{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07. å¹¶è¡Œæ‰§è¡Œå’ŒMap-Reduce\n",
    "\n",
    "## è¯¾ç¨‹ç›®æ ‡\n",
    "- æŒæ¡å¹¶è¡ŒèŠ‚ç‚¹æ‰§è¡Œæœºåˆ¶\n",
    "- å­¦ä¹ Map-Reduceæ¨¡å¼å®ç°\n",
    "- ä½¿ç”¨Send APIåˆ›å»ºåŠ¨æ€å¹¶è¡Œä»»åŠ¡\n",
    "- å®ç°ç»“æœèšåˆå’ŒåŒæ­¥ç­–ç•¥\n",
    "- ä¼˜åŒ–å¹¶è¡Œå¤„ç†æ€§èƒ½\n",
    "\n",
    "## æ ¸å¿ƒæ¦‚å¿µ\n",
    "\n",
    "LangGraphæ”¯æŒå¼ºå¤§çš„å¹¶è¡Œå¤„ç†èƒ½åŠ›ï¼š\n",
    "1. **å¹¶è¡ŒèŠ‚ç‚¹**ï¼šåŒæ—¶æ‰§è¡Œå¤šä¸ªèŠ‚ç‚¹\n",
    "2. **Map-Reduce**ï¼šåˆ†å¸ƒå¼å¤„ç†æ¨¡å¼\n",
    "3. **Send API**ï¼šåŠ¨æ€åˆ›å»ºå¹¶è¡Œä»»åŠ¡\n",
    "4. **ç»“æœèšåˆ**ï¼šæ”¶é›†å’Œåˆå¹¶å¹¶è¡Œç»“æœ\n",
    "5. **è´Ÿè½½å‡è¡¡**ï¼šä¼˜åŒ–èµ„æºåˆ©ç”¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç¯å¢ƒå‡†å¤‡\n",
    "from typing import TypedDict, Annotated, List, Dict, Any\n",
    "from langgraph.graph import StateGraph, END, START\n",
    "from langgraph.constants import Send\n",
    "import asyncio\n",
    "import time\n",
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import random\n",
    "\n",
    "print(\"ç¯å¢ƒå‡†å¤‡å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. åŸºç¡€å¹¶è¡Œæ‰§è¡Œ\n",
    "\n",
    "è®©æˆ‘ä»¬ä»ç®€å•çš„å¹¶è¡Œæ‰§è¡Œå¼€å§‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¹¶è¡Œå¤„ç†çŠ¶æ€\n",
    "class ParallelState(TypedDict):\n",
    "    input_data: List[str]\n",
    "    results: List[Dict[str, Any]]\n",
    "    processing_time: float\n",
    "    total_items: int\n",
    "\n",
    "# å¹¶è¡Œå·¥ä½œèŠ‚ç‚¹\n",
    "def worker_a(state: ParallelState) -> ParallelState:\n",
    "    \"\"\"å·¥ä½œèŠ‚ç‚¹A\"\"\"\n",
    "    print(\"ğŸ”§ Worker A å¼€å§‹å¤„ç†...\")\n",
    "    time.sleep(1)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´\n",
    "    \n",
    "    result = {\n",
    "        \"worker\": \"A\",\n",
    "        \"processed_items\": 3,\n",
    "        \"status\": \"completed\",\n",
    "        \"result\": \"Aå¤„ç†å®Œæˆ\"\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        \"results\": state.get(\"results\", []) + [result]\n",
    "    }\n",
    "\n",
    "def worker_b(state: ParallelState) -> ParallelState:\n",
    "    \"\"\"å·¥ä½œèŠ‚ç‚¹B\"\"\"\n",
    "    print(\"âš™ï¸ Worker B å¼€å§‹å¤„ç†...\")\n",
    "    time.sleep(1.5)  # ä¸åŒçš„å¤„ç†æ—¶é—´\n",
    "    \n",
    "    result = {\n",
    "        \"worker\": \"B\",\n",
    "        \"processed_items\": 5,\n",
    "        \"status\": \"completed\",\n",
    "        \"result\": \"Bå¤„ç†å®Œæˆ\"\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        \"results\": state.get(\"results\", []) + [result]\n",
    "    }\n",
    "\n",
    "def worker_c(state: ParallelState) -> ParallelState:\n",
    "    \"\"\"å·¥ä½œèŠ‚ç‚¹C\"\"\"\n",
    "    print(\"ğŸ› ï¸ Worker C å¼€å§‹å¤„ç†...\")\n",
    "    time.sleep(0.8)\n",
    "    \n",
    "    result = {\n",
    "        \"worker\": \"C\",\n",
    "        \"processed_items\": 2,\n",
    "        \"status\": \"completed\",\n",
    "        \"result\": \"Cå¤„ç†å®Œæˆ\"\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        \"results\": state.get(\"results\", []) + [result]\n",
    "    }\n",
    "\n",
    "def aggregate_results(state: ParallelState) -> ParallelState:\n",
    "    \"\"\"èšåˆç»“æœ\"\"\"\n",
    "    results = state.get(\"results\", [])\n",
    "    total_processed = sum(r.get(\"processed_items\", 0) for r in results)\n",
    "    \n",
    "    print(f\"ğŸ“Š èšåˆå®Œæˆ: {len(results)} ä¸ªå·¥ä½œèŠ‚ç‚¹, æ€»å¤„ç† {total_processed} é¡¹\")\n",
    "    \n",
    "    return {\n",
    "        \"total_items\": total_processed\n",
    "    }\n",
    "\n",
    "# åˆ›å»ºå¹¶è¡Œæ‰§è¡Œå›¾\n",
    "def create_parallel_graph():\n",
    "    graph = StateGraph(ParallelState)\n",
    "    \n",
    "    # æ·»åŠ å¹¶è¡Œå·¥ä½œèŠ‚ç‚¹\n",
    "    graph.add_node(\"worker_a\", worker_a)\n",
    "    graph.add_node(\"worker_b\", worker_b)\n",
    "    graph.add_node(\"worker_c\", worker_c)\n",
    "    graph.add_node(\"aggregate\", aggregate_results)\n",
    "    \n",
    "    # è®¾ç½®å¹¶è¡Œå…¥å£ç‚¹\n",
    "    graph.set_entry_point(\"worker_a\")\n",
    "    graph.set_entry_point(\"worker_b\")\n",
    "    graph.set_entry_point(\"worker_c\")\n",
    "    \n",
    "    # æ‰€æœ‰å·¥ä½œèŠ‚ç‚¹å®Œæˆåèšåˆ\n",
    "    graph.add_edge(\"worker_a\", \"aggregate\")\n",
    "    graph.add_edge(\"worker_b\", \"aggregate\")\n",
    "    graph.add_edge(\"worker_c\", \"aggregate\")\n",
    "    graph.add_edge(\"aggregate\", END)\n",
    "    \n",
    "    return graph.compile()\n",
    "\n",
    "# æµ‹è¯•å¹¶è¡Œæ‰§è¡Œ\n",
    "parallel_app = create_parallel_graph()\n",
    "\n",
    "print(\"=== å¹¶è¡Œæ‰§è¡Œæ¼”ç¤º ===\")\n",
    "start_time = time.time()\n",
    "\n",
    "result = parallel_app.invoke({\n",
    "    \"input_data\": [\"data1\", \"data2\", \"data3\"],\n",
    "    \"results\": []\n",
    "})\n",
    "\n",
    "execution_time = time.time() - start_time\n",
    "print(f\"\\næ‰§è¡Œæ—¶é—´: {execution_time:.2f} ç§’\")\n",
    "print(f\"å¤„ç†ç»“æœ: {len(result.get('results', []))} ä¸ªå·¥ä½œèŠ‚ç‚¹å®Œæˆ\")\n",
    "print(f\"æ€»å¤„ç†é¡¹ç›®: {result.get('total_items', 0)} é¡¹\")\n",
    "\n",
    "# æ˜¾ç¤ºè¯¦ç»†ç»“æœ\n",
    "for r in result.get('results', []):\n",
    "    print(f\"- {r['worker']}: {r['result']} ({r['processed_items']} é¡¹)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Map-Reduce æ¨¡å¼\n",
    "\n",
    "å®ç°ç»å…¸çš„Map-Reduceå¤„ç†æ¨¡å¼ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map-Reduce çŠ¶æ€\n",
    "class MapReduceState(TypedDict):\n",
    "    input_data: List[int]\n",
    "    map_results: List[Dict[str, Any]]\n",
    "    reduce_result: Dict[str, Any]\n",
    "    chunk_size: int\n",
    "\n",
    "def split_data(state: MapReduceState) -> MapReduceState:\n",
    "    \"\"\"åˆ†å‰²æ•°æ®\"\"\"\n",
    "    input_data = state.get(\"input_data\", [])\n",
    "    chunk_size = state.get(\"chunk_size\", 3)\n",
    "    \n",
    "    # å°†æ•°æ®åˆ†æˆå—\n",
    "    chunks = []\n",
    "    for i in range(0, len(input_data), chunk_size):\n",
    "        chunk = input_data[i:i + chunk_size]\n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    print(f\"ğŸ“¦ æ•°æ®åˆ†å‰²: {len(input_data)} é¡¹åˆ†æˆ {len(chunks)} å—\")\n",
    "    \n",
    "    return {\"chunks\": chunks}\n",
    "\n",
    "# åˆ›å»ºåŠ¨æ€MapèŠ‚ç‚¹\n",
    "def create_map_tasks(state: MapReduceState) -> List[Send]:\n",
    "    \"\"\"åˆ›å»ºMapä»»åŠ¡\"\"\"\n",
    "    chunks = state.get(\"chunks\", [])\n",
    "    \n",
    "    # ä¸ºæ¯ä¸ªæ•°æ®å—åˆ›å»ºä¸€ä¸ªMapä»»åŠ¡\n",
    "    tasks = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        task = Send(\"map_worker\", {\n",
    "            \"chunk_id\": i,\n",
    "            \"chunk_data\": chunk\n",
    "        })\n",
    "        tasks.append(task)\n",
    "    \n",
    "    print(f\"ğŸš€ åˆ›å»º {len(tasks)} ä¸ªMapä»»åŠ¡\")\n",
    "    return tasks\n",
    "\n",
    "def map_worker(state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Mapå·¥ä½œèŠ‚ç‚¹\"\"\"\n",
    "    chunk_id = state[\"chunk_id\"]\n",
    "    chunk_data = state[\"chunk_data\"]\n",
    "    \n",
    "    print(f\"ğŸ”„ Map {chunk_id}: å¤„ç† {len(chunk_data)} ä¸ªæ•°æ®é¡¹\")\n",
    "    \n",
    "    # æ¨¡æ‹ŸMapæ“ä½œï¼šè®¡ç®—å¹³æ–¹å’Œ\n",
    "    squared_sum = sum(x * x for x in chunk_data)\n",
    "    max_value = max(chunk_data) if chunk_data else 0\n",
    "    min_value = min(chunk_data) if chunk_data else 0\n",
    "    \n",
    "    # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´\n",
    "    time.sleep(0.5)\n",
    "    \n",
    "    result = {\n",
    "        \"chunk_id\": chunk_id,\n",
    "        \"squared_sum\": squared_sum,\n",
    "        \"max_value\": max_value,\n",
    "        \"min_value\": min_value,\n",
    "        \"count\": len(chunk_data)\n",
    "    }\n",
    "    \n",
    "    print(f\"âœ… Map {chunk_id} å®Œæˆ: å¹³æ–¹å’Œ={squared_sum}\")\n",
    "    \n",
    "    return {\n",
    "        \"map_results\": [result]\n",
    "    }\n",
    "\n",
    "def reduce_worker(state: MapReduceState) -> MapReduceState:\n",
    "    \"\"\"Reduceå·¥ä½œèŠ‚ç‚¹\"\"\"\n",
    "    map_results = state.get(\"map_results\", [])\n",
    "    \n",
    "    print(f\"ğŸ”„ Reduce: èšåˆ {len(map_results)} ä¸ªMapç»“æœ\")\n",
    "    \n",
    "    # èšåˆæ‰€æœ‰Mapç»“æœ\n",
    "    total_squared_sum = sum(r[\"squared_sum\"] for r in map_results)\n",
    "    global_max = max(r[\"max_value\"] for r in map_results) if map_results else 0\n",
    "    global_min = min(r[\"min_value\"] for r in map_results) if map_results else 0\n",
    "    total_count = sum(r[\"count\"] for r in map_results)\n",
    "    \n",
    "    reduce_result = {\n",
    "        \"total_squared_sum\": total_squared_sum,\n",
    "        \"global_max\": global_max,\n",
    "        \"global_min\": global_min,\n",
    "        \"total_count\": total_count,\n",
    "        \"average_squared\": total_squared_sum / total_count if total_count > 0 else 0\n",
    "    }\n",
    "    \n",
    "    print(f\"âœ… Reduce å®Œæˆ: æ€»å¹³æ–¹å’Œ={total_squared_sum}, å…¨å±€æœ€å¤§å€¼={global_max}\")\n",
    "    \n",
    "    return {\"reduce_result\": reduce_result}\n",
    "\n",
    "# åˆ›å»ºMap-Reduceå›¾\n",
    "def create_mapreduce_graph():\n",
    "    graph = StateGraph(MapReduceState)\n",
    "    \n",
    "    # æ·»åŠ èŠ‚ç‚¹\n",
    "    graph.add_node(\"split\", split_data)\n",
    "    graph.add_node(\"map_worker\", map_worker)\n",
    "    graph.add_node(\"reduce\", reduce_worker)\n",
    "    \n",
    "    # è®¾ç½®æµç¨‹\n",
    "    graph.set_entry_point(\"split\")\n",
    "    graph.add_conditional_edges(\"split\", create_map_tasks, [\"map_worker\"])\n",
    "    graph.add_edge(\"map_worker\", \"reduce\")\n",
    "    graph.add_edge(\"reduce\", END)\n",
    "    \n",
    "    return graph.compile()\n",
    "\n",
    "# æµ‹è¯•Map-Reduce\n",
    "mapreduce_app = create_mapreduce_graph()\n",
    "\n",
    "print(\"\\n=== Map-Reduce æ¼”ç¤º ===\")\n",
    "test_data = list(range(1, 21))  # 1åˆ°20çš„æ•°å­—\n",
    "print(f\"è¾“å…¥æ•°æ®: {test_data}\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "mapreduce_result = mapreduce_app.invoke({\n",
    "    \"input_data\": test_data,\n",
    "    \"chunk_size\": 5\n",
    "})\n",
    "\n",
    "execution_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n=== Map-Reduce ç»“æœ ===\")\n",
    "print(f\"æ‰§è¡Œæ—¶é—´: {execution_time:.2f} ç§’\")\n",
    "\n",
    "reduce_result = mapreduce_result.get(\"reduce_result\", {})\n",
    "print(f\"æ€»å¹³æ–¹å’Œ: {reduce_result.get('total_squared_sum', 0)}\")\n",
    "print(f\"å…¨å±€æœ€å¤§å€¼: {reduce_result.get('global_max', 0)}\")\n",
    "print(f\"å…¨å±€æœ€å°å€¼: {reduce_result.get('global_min', 0)}\")\n",
    "print(f\"å¹³å‡å¹³æ–¹å€¼: {reduce_result.get('average_squared', 0):.2f}\")\n",
    "print(f\"å¤„ç†æ€»æ•°: {reduce_result.get('total_count', 0)} é¡¹\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. å®è·µæ¡ˆä¾‹ï¼šå¤§è§„æ¨¡æ–‡æœ¬å¤„ç†\n",
    "\n",
    "æ„å»ºä¸€ä¸ªå¤§è§„æ¨¡æ–‡æœ¬å¤„ç†ç³»ç»Ÿï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ–‡æœ¬å¤„ç†çŠ¶æ€\n",
    "class TextProcessingState(TypedDict):\n",
    "    documents: List[str]\n",
    "    processing_results: List[Dict[str, Any]]\n",
    "    final_statistics: Dict[str, Any]\n",
    "    batch_size: int\n",
    "\n",
    "def prepare_documents(state: TextProcessingState) -> TextProcessingState:\n",
    "    \"\"\"å‡†å¤‡æ–‡æ¡£æ‰¹æ¬¡\"\"\"\n",
    "    # æ¨¡æ‹Ÿå¤§é‡æ–‡æ¡£\n",
    "    documents = [\n",
    "        \"This is document 1. It contains some text for analysis.\",\n",
    "        \"Document 2 has different content and more words to process.\",\n",
    "        \"The third document discusses various topics and concepts.\",\n",
    "        \"Document 4 contains technical information about systems.\",\n",
    "        \"Fifth document explores advanced algorithmic approaches.\",\n",
    "        \"Document 6 covers machine learning and data science.\",\n",
    "        \"The seventh document examines distributed computing.\",\n",
    "        \"Document 8 focuses on parallel processing techniques.\",\n",
    "        \"Ninth document analyzes performance optimization methods.\",\n",
    "        \"The final document summarizes key findings and conclusions.\"\n",
    "    ]\n",
    "    \n",
    "    print(f\"ğŸ“š å‡†å¤‡äº† {len(documents)} ä¸ªæ–‡æ¡£è¿›è¡Œå¤„ç†\")\n",
    "    \n",
    "    return {\n",
    "        \"documents\": documents,\n",
    "        \"batch_size\": 3\n",
    "    }\n",
    "\n",
    "def create_processing_tasks(state: TextProcessingState) -> List[Send]:\n",
    "    \"\"\"åˆ›å»ºæ–‡æœ¬å¤„ç†ä»»åŠ¡\"\"\"\n",
    "    documents = state.get(\"documents\", [])\n",
    "    batch_size = state.get(\"batch_size\", 3)\n",
    "    \n",
    "    # å°†æ–‡æ¡£åˆ†æ‰¹\n",
    "    tasks = []\n",
    "    for i in range(0, len(documents), batch_size):\n",
    "        batch = documents[i:i + batch_size]\n",
    "        task = Send(\"process_batch\", {\n",
    "            \"batch_id\": i // batch_size,\n",
    "            \"batch_documents\": batch\n",
    "        })\n",
    "        tasks.append(task)\n",
    "    \n",
    "    print(f\"ğŸš€ åˆ›å»º {len(tasks)} ä¸ªå¤„ç†æ‰¹æ¬¡\")\n",
    "    return tasks\n",
    "\n",
    "def process_batch(state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"å¤„ç†æ–‡æ¡£æ‰¹æ¬¡\"\"\"\n",
    "    batch_id = state[\"batch_id\"]\n",
    "    documents = state[\"batch_documents\"]\n",
    "    \n",
    "    print(f\"ğŸ“„ æ‰¹æ¬¡ {batch_id}: å¤„ç† {len(documents)} ä¸ªæ–‡æ¡£\")\n",
    "    \n",
    "    # æ¨¡æ‹Ÿæ–‡æœ¬åˆ†æ\n",
    "    batch_stats = {\n",
    "        \"batch_id\": batch_id,\n",
    "        \"document_count\": len(documents),\n",
    "        \"total_words\": 0,\n",
    "        \"total_characters\": 0,\n",
    "        \"word_frequencies\": {},\n",
    "        \"avg_doc_length\": 0\n",
    "    }\n",
    "    \n",
    "    all_words = []\n",
    "    total_chars = 0\n",
    "    \n",
    "    for doc in documents:\n",
    "        # ç®€å•çš„æ–‡æœ¬å¤„ç†\n",
    "        words = doc.lower().replace('.', '').replace(',', '').split()\n",
    "        all_words.extend(words)\n",
    "        total_chars += len(doc)\n",
    "    \n",
    "    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯\n",
    "    word_freq = {}\n",
    "    for word in all_words:\n",
    "        word_freq[word] = word_freq.get(word, 0) + 1\n",
    "    \n",
    "    batch_stats.update({\n",
    "        \"total_words\": len(all_words),\n",
    "        \"total_characters\": total_chars,\n",
    "        \"word_frequencies\": dict(list(word_freq.items())[:10]),  # åªä¿ç•™å‰10ä¸ª\n",
    "        \"avg_doc_length\": total_chars / len(documents) if documents else 0\n",
    "    })\n",
    "    \n",
    "    # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´\n",
    "    time.sleep(0.8)\n",
    "    \n",
    "    print(f\"âœ… æ‰¹æ¬¡ {batch_id} å®Œæˆ: {len(all_words)} ä¸ªå•è¯, {total_chars} ä¸ªå­—ç¬¦\")\n",
    "    \n",
    "    return {\n",
    "        \"processing_results\": [batch_stats]\n",
    "    }\n",
    "\n",
    "def aggregate_statistics(state: TextProcessingState) -> TextProcessingState:\n",
    "    \"\"\"èšåˆç»Ÿè®¡ä¿¡æ¯\"\"\"\n",
    "    results = state.get(\"processing_results\", [])\n",
    "    \n",
    "    print(f\"ğŸ“Š èšåˆ {len(results)} ä¸ªæ‰¹æ¬¡çš„ç»“æœ\")\n",
    "    \n",
    "    # åˆå¹¶æ‰€æœ‰ç»Ÿè®¡ä¿¡æ¯\n",
    "    total_docs = sum(r[\"document_count\"] for r in results)\n",
    "    total_words = sum(r[\"total_words\"] for r in results)\n",
    "    total_chars = sum(r[\"total_characters\"] for r in results)\n",
    "    \n",
    "    # åˆå¹¶è¯é¢‘\n",
    "    combined_freq = {}\n",
    "    for result in results:\n",
    "        for word, freq in result[\"word_frequencies\"].items():\n",
    "            combined_freq[word] = combined_freq.get(word, 0) + freq\n",
    "    \n",
    "    # è·å–æœ€é«˜é¢‘è¯æ±‡\n",
    "    top_words = sorted(combined_freq.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    \n",
    "    final_stats = {\n",
    "        \"total_documents\": total_docs,\n",
    "        \"total_words\": total_words,\n",
    "        \"total_characters\": total_chars,\n",
    "        \"avg_words_per_doc\": total_words / total_docs if total_docs > 0 else 0,\n",
    "        \"avg_chars_per_doc\": total_chars / total_docs if total_docs > 0 else 0,\n",
    "        \"top_10_words\": top_words,\n",
    "        \"processing_batches\": len(results)\n",
    "    }\n",
    "    \n",
    "    print(f\"âœ… èšåˆå®Œæˆ: {total_docs} æ–‡æ¡£, {total_words} å•è¯\")\n",
    "    \n",
    "    return {\"final_statistics\": final_stats}\n",
    "\n",
    "# åˆ›å»ºæ–‡æœ¬å¤„ç†å›¾\n",
    "def create_text_processing_graph():\n",
    "    graph = StateGraph(TextProcessingState)\n",
    "    \n",
    "    graph.add_node(\"prepare\", prepare_documents)\n",
    "    graph.add_node(\"process_batch\", process_batch)\n",
    "    graph.add_node(\"aggregate\", aggregate_statistics)\n",
    "    \n",
    "    graph.set_entry_point(\"prepare\")\n",
    "    graph.add_conditional_edges(\"prepare\", create_processing_tasks, [\"process_batch\"])\n",
    "    graph.add_edge(\"process_batch\", \"aggregate\")\n",
    "    graph.add_edge(\"aggregate\", END)\n",
    "    \n",
    "    return graph.compile()\n",
    "\n",
    "# æµ‹è¯•æ–‡æœ¬å¤„ç†ç³»ç»Ÿ\n",
    "text_app = create_text_processing_graph()\n",
    "\n",
    "print(\"\\n=== å¤§è§„æ¨¡æ–‡æœ¬å¤„ç†æ¼”ç¤º ===\")\n",
    "start_time = time.time()\n",
    "\n",
    "text_result = text_app.invoke({})\n",
    "\n",
    "execution_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n=== æ–‡æœ¬å¤„ç†ç»“æœ ===\")\n",
    "print(f\"æ€»æ‰§è¡Œæ—¶é—´: {execution_time:.2f} ç§’\")\n",
    "\n",
    "stats = text_result.get(\"final_statistics\", {})\n",
    "print(f\"å¤„ç†æ–‡æ¡£æ•°: {stats.get('total_documents', 0)}\")\n",
    "print(f\"æ€»å•è¯æ•°: {stats.get('total_words', 0)}\")\n",
    "print(f\"æ€»å­—ç¬¦æ•°: {stats.get('total_characters', 0)}\")\n",
    "print(f\"å¹³å‡æ¯æ–‡æ¡£å•è¯æ•°: {stats.get('avg_words_per_doc', 0):.1f}\")\n",
    "print(f\"å¤„ç†æ‰¹æ¬¡æ•°: {stats.get('processing_batches', 0)}\")\n",
    "\n",
    "print(\"\\né«˜é¢‘è¯æ±‡:\")\n",
    "for word, freq in stats.get('top_10_words', [])[:5]:\n",
    "    print(f\"  {word}: {freq} æ¬¡\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ç»ƒä¹ é¢˜\n",
    "\n",
    "### ç»ƒä¹ 1ï¼šå¹¶è¡Œå›¾åƒå¤„ç†ç³»ç»Ÿ\n",
    "åˆ›å»ºä¸€ä¸ªå¹¶è¡Œå›¾åƒå¤„ç†ç³»ç»Ÿï¼Œæ”¯æŒå¤šç§æ»¤é•œæ•ˆæœï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç»ƒä¹ 1ï¼šè¯·å®ç°å¹¶è¡Œå›¾åƒå¤„ç†ç³»ç»Ÿ\n",
    "# TODO: åˆ›å»ºå¤šä¸ªå›¾åƒæ»¤é•œèŠ‚ç‚¹\n",
    "# TODO: å®ç°å¹¶è¡Œå¤„ç†å¤šå¼ å›¾ç‰‡\n",
    "# TODO: æ”¯æŒä¸åŒçš„å¤„ç†æ•ˆæœç»„åˆ\n",
    "\n",
    "print(\"è¯·å®ç°å¹¶è¡Œå›¾åƒå¤„ç†ç³»ç»Ÿ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ç»ƒä¹ 2ï¼šåˆ†å¸ƒå¼æ•°æ®åˆ†æ\n",
    "æ„å»ºä¸€ä¸ªåˆ†å¸ƒå¼æ•°æ®åˆ†æç³»ç»Ÿï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç»ƒä¹ 2ï¼šè¯·å®ç°åˆ†å¸ƒå¼æ•°æ®åˆ†æç³»ç»Ÿ\n",
    "# TODO: å®ç°æ•°æ®åˆ†ç‰‡å’Œåˆ†å‘\n",
    "# TODO: å¹¶è¡Œç»Ÿè®¡åˆ†æ\n",
    "# TODO: ç»“æœèšåˆå’ŒæŠ¥å‘Šç”Ÿæˆ\n",
    "\n",
    "print(\"è¯·å®ç°åˆ†å¸ƒå¼æ•°æ®åˆ†æç³»ç»Ÿ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ€»ç»“\n",
    "\n",
    "åœ¨æœ¬è¯¾ä¸­ï¼Œæˆ‘ä»¬å­¦ä¹ äº†å¹¶è¡Œæ‰§è¡Œå’ŒMap-Reduceæ¨¡å¼ï¼š\n",
    "\n",
    "### å…³é”®è¦ç‚¹ï¼š\n",
    "1. **å¹¶è¡Œæ‰§è¡Œ**ï¼šåŒæ—¶è¿è¡Œå¤šä¸ªèŠ‚ç‚¹æé«˜æ•ˆç‡\n",
    "2. **Map-Reduce**ï¼šåˆ†è€Œæ²»ä¹‹çš„å¤„ç†æ¨¡å¼\n",
    "3. **Send API**ï¼šåŠ¨æ€åˆ›å»ºå¹¶è¡Œä»»åŠ¡\n",
    "4. **ç»“æœèšåˆ**ï¼šåˆå¹¶å¹¶è¡Œå¤„ç†ç»“æœ\n",
    "5. **æ€§èƒ½ä¼˜åŒ–**ï¼šåˆç†è®¾è®¡å¹¶è¡Œåº¦å’Œæ‰¹æ¬¡å¤§å°\n",
    "\n",
    "### æœ€ä½³å®è·µï¼š\n",
    "- **åˆç†åˆ†ç‰‡**ï¼šå¹³è¡¡è´Ÿè½½å’Œé€šä¿¡å¼€é”€\n",
    "- **é”™è¯¯å¤„ç†**ï¼šå¤„ç†å¹¶è¡Œä»»åŠ¡ä¸­çš„å¼‚å¸¸\n",
    "- **èµ„æºç®¡ç†**ï¼šæ§åˆ¶å¹¶å‘åº¦é¿å…èµ„æºè€—å°½\n",
    "- **ç›‘æ§è°ƒè¯•**ï¼šè·Ÿè¸ªå¹¶è¡Œä»»åŠ¡æ‰§è¡ŒçŠ¶æ€\n",
    "\n",
    "### åº”ç”¨åœºæ™¯ï¼š\n",
    "- å¤§æ•°æ®å¤„ç†\n",
    "- å›¾åƒ/è§†é¢‘å¤„ç†\n",
    "- ç§‘å­¦è®¡ç®—\n",
    "- æœºå™¨å­¦ä¹ è®­ç»ƒ\n",
    "- çˆ¬è™«å’Œæ•°æ®é‡‡é›†\n",
    "\n",
    "## ä¸‹ä¸€è¯¾é¢„å‘Š\n",
    "\n",
    "åœ¨ä¸‹ä¸€è¯¾ã€ŠHuman-in-the-Loopã€‹ä¸­ï¼Œæˆ‘ä»¬å°†å­¦ä¹ ï¼š\n",
    "- äººæœºäº¤äº’èŠ‚ç‚¹è®¾è®¡\n",
    "- ä¸­æ–­æœºåˆ¶å®ç°\n",
    "- äººå·¥å®¡æ‰¹æµç¨‹\n",
    "- è¾“å…¥éªŒè¯å’Œç¡®è®¤\n",
    "- äº¤äº’å¼å¯¹è¯ç³»ç»Ÿæ„å»º"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}