{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07. 并行执行和Map-Reduce\n",
    "\n",
    "## 课程目标\n",
    "- 掌握并行节点执行机制\n",
    "- 学习Map-Reduce模式实现\n",
    "- 使用Send API创建动态并行任务\n",
    "- 实现结果聚合和同步策略\n",
    "- 优化并行处理性能\n",
    "\n",
    "## 核心概念\n",
    "\n",
    "LangGraph支持强大的并行处理能力：\n",
    "1. **并行节点**：同时执行多个节点\n",
    "2. **Map-Reduce**：分布式处理模式\n",
    "3. **Send API**：动态创建并行任务\n",
    "4. **结果聚合**：收集和合并并行结果\n",
    "5. **负载均衡**：优化资源利用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 环境准备\n",
    "from typing import TypedDict, Annotated, List, Dict, Any\n",
    "from langgraph.graph import StateGraph, END, START\n",
    "from langgraph.constants import Send\n",
    "import asyncio\n",
    "import time\n",
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import random\n",
    "\n",
    "print(\"环境准备完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 基础并行执行\n",
    "\n",
    "让我们从简单的并行执行开始："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 并行处理状态\n",
    "class ParallelState(TypedDict):\n",
    "    input_data: List[str]\n",
    "    results: List[Dict[str, Any]]\n",
    "    processing_time: float\n",
    "    total_items: int\n",
    "\n",
    "# 并行工作节点\n",
    "def worker_a(state: ParallelState) -> ParallelState:\n",
    "    \"\"\"工作节点A\"\"\"\n",
    "    print(\"🔧 Worker A 开始处理...\")\n",
    "    time.sleep(1)  # 模拟处理时间\n",
    "    \n",
    "    result = {\n",
    "        \"worker\": \"A\",\n",
    "        \"processed_items\": 3,\n",
    "        \"status\": \"completed\",\n",
    "        \"result\": \"A处理完成\"\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        \"results\": state.get(\"results\", []) + [result]\n",
    "    }\n",
    "\n",
    "def worker_b(state: ParallelState) -> ParallelState:\n",
    "    \"\"\"工作节点B\"\"\"\n",
    "    print(\"⚙️ Worker B 开始处理...\")\n",
    "    time.sleep(1.5)  # 不同的处理时间\n",
    "    \n",
    "    result = {\n",
    "        \"worker\": \"B\",\n",
    "        \"processed_items\": 5,\n",
    "        \"status\": \"completed\",\n",
    "        \"result\": \"B处理完成\"\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        \"results\": state.get(\"results\", []) + [result]\n",
    "    }\n",
    "\n",
    "def worker_c(state: ParallelState) -> ParallelState:\n",
    "    \"\"\"工作节点C\"\"\"\n",
    "    print(\"🛠️ Worker C 开始处理...\")\n",
    "    time.sleep(0.8)\n",
    "    \n",
    "    result = {\n",
    "        \"worker\": \"C\",\n",
    "        \"processed_items\": 2,\n",
    "        \"status\": \"completed\",\n",
    "        \"result\": \"C处理完成\"\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        \"results\": state.get(\"results\", []) + [result]\n",
    "    }\n",
    "\n",
    "def aggregate_results(state: ParallelState) -> ParallelState:\n",
    "    \"\"\"聚合结果\"\"\"\n",
    "    results = state.get(\"results\", [])\n",
    "    total_processed = sum(r.get(\"processed_items\", 0) for r in results)\n",
    "    \n",
    "    print(f\"📊 聚合完成: {len(results)} 个工作节点, 总处理 {total_processed} 项\")\n",
    "    \n",
    "    return {\n",
    "        \"total_items\": total_processed\n",
    "    }\n",
    "\n",
    "# 创建并行执行图\n",
    "def create_parallel_graph():\n",
    "    graph = StateGraph(ParallelState)\n",
    "    \n",
    "    # 添加并行工作节点\n",
    "    graph.add_node(\"worker_a\", worker_a)\n",
    "    graph.add_node(\"worker_b\", worker_b)\n",
    "    graph.add_node(\"worker_c\", worker_c)\n",
    "    graph.add_node(\"aggregate\", aggregate_results)\n",
    "    \n",
    "    # 设置并行入口点\n",
    "    graph.set_entry_point(\"worker_a\")\n",
    "    graph.set_entry_point(\"worker_b\")\n",
    "    graph.set_entry_point(\"worker_c\")\n",
    "    \n",
    "    # 所有工作节点完成后聚合\n",
    "    graph.add_edge(\"worker_a\", \"aggregate\")\n",
    "    graph.add_edge(\"worker_b\", \"aggregate\")\n",
    "    graph.add_edge(\"worker_c\", \"aggregate\")\n",
    "    graph.add_edge(\"aggregate\", END)\n",
    "    \n",
    "    return graph.compile()\n",
    "\n",
    "# 测试并行执行\n",
    "parallel_app = create_parallel_graph()\n",
    "\n",
    "print(\"=== 并行执行演示 ===\")\n",
    "start_time = time.time()\n",
    "\n",
    "result = parallel_app.invoke({\n",
    "    \"input_data\": [\"data1\", \"data2\", \"data3\"],\n",
    "    \"results\": []\n",
    "})\n",
    "\n",
    "execution_time = time.time() - start_time\n",
    "print(f\"\\n执行时间: {execution_time:.2f} 秒\")\n",
    "print(f\"处理结果: {len(result.get('results', []))} 个工作节点完成\")\n",
    "print(f\"总处理项目: {result.get('total_items', 0)} 项\")\n",
    "\n",
    "# 显示详细结果\n",
    "for r in result.get('results', []):\n",
    "    print(f\"- {r['worker']}: {r['result']} ({r['processed_items']} 项)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Map-Reduce 模式\n",
    "\n",
    "实现经典的Map-Reduce处理模式："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map-Reduce 状态\n",
    "class MapReduceState(TypedDict):\n",
    "    input_data: List[int]\n",
    "    map_results: List[Dict[str, Any]]\n",
    "    reduce_result: Dict[str, Any]\n",
    "    chunk_size: int\n",
    "\n",
    "def split_data(state: MapReduceState) -> MapReduceState:\n",
    "    \"\"\"分割数据\"\"\"\n",
    "    input_data = state.get(\"input_data\", [])\n",
    "    chunk_size = state.get(\"chunk_size\", 3)\n",
    "    \n",
    "    # 将数据分成块\n",
    "    chunks = []\n",
    "    for i in range(0, len(input_data), chunk_size):\n",
    "        chunk = input_data[i:i + chunk_size]\n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    print(f\"📦 数据分割: {len(input_data)} 项分成 {len(chunks)} 块\")\n",
    "    \n",
    "    return {\"chunks\": chunks}\n",
    "\n",
    "# 创建动态Map节点\n",
    "def create_map_tasks(state: MapReduceState) -> List[Send]:\n",
    "    \"\"\"创建Map任务\"\"\"\n",
    "    chunks = state.get(\"chunks\", [])\n",
    "    \n",
    "    # 为每个数据块创建一个Map任务\n",
    "    tasks = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        task = Send(\"map_worker\", {\n",
    "            \"chunk_id\": i,\n",
    "            \"chunk_data\": chunk\n",
    "        })\n",
    "        tasks.append(task)\n",
    "    \n",
    "    print(f\"🚀 创建 {len(tasks)} 个Map任务\")\n",
    "    return tasks\n",
    "\n",
    "def map_worker(state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Map工作节点\"\"\"\n",
    "    chunk_id = state[\"chunk_id\"]\n",
    "    chunk_data = state[\"chunk_data\"]\n",
    "    \n",
    "    print(f\"🔄 Map {chunk_id}: 处理 {len(chunk_data)} 个数据项\")\n",
    "    \n",
    "    # 模拟Map操作：计算平方和\n",
    "    squared_sum = sum(x * x for x in chunk_data)\n",
    "    max_value = max(chunk_data) if chunk_data else 0\n",
    "    min_value = min(chunk_data) if chunk_data else 0\n",
    "    \n",
    "    # 模拟处理时间\n",
    "    time.sleep(0.5)\n",
    "    \n",
    "    result = {\n",
    "        \"chunk_id\": chunk_id,\n",
    "        \"squared_sum\": squared_sum,\n",
    "        \"max_value\": max_value,\n",
    "        \"min_value\": min_value,\n",
    "        \"count\": len(chunk_data)\n",
    "    }\n",
    "    \n",
    "    print(f\"✅ Map {chunk_id} 完成: 平方和={squared_sum}\")\n",
    "    \n",
    "    return {\n",
    "        \"map_results\": [result]\n",
    "    }\n",
    "\n",
    "def reduce_worker(state: MapReduceState) -> MapReduceState:\n",
    "    \"\"\"Reduce工作节点\"\"\"\n",
    "    map_results = state.get(\"map_results\", [])\n",
    "    \n",
    "    print(f\"🔄 Reduce: 聚合 {len(map_results)} 个Map结果\")\n",
    "    \n",
    "    # 聚合所有Map结果\n",
    "    total_squared_sum = sum(r[\"squared_sum\"] for r in map_results)\n",
    "    global_max = max(r[\"max_value\"] for r in map_results) if map_results else 0\n",
    "    global_min = min(r[\"min_value\"] for r in map_results) if map_results else 0\n",
    "    total_count = sum(r[\"count\"] for r in map_results)\n",
    "    \n",
    "    reduce_result = {\n",
    "        \"total_squared_sum\": total_squared_sum,\n",
    "        \"global_max\": global_max,\n",
    "        \"global_min\": global_min,\n",
    "        \"total_count\": total_count,\n",
    "        \"average_squared\": total_squared_sum / total_count if total_count > 0 else 0\n",
    "    }\n",
    "    \n",
    "    print(f\"✅ Reduce 完成: 总平方和={total_squared_sum}, 全局最大值={global_max}\")\n",
    "    \n",
    "    return {\"reduce_result\": reduce_result}\n",
    "\n",
    "# 创建Map-Reduce图\n",
    "def create_mapreduce_graph():\n",
    "    graph = StateGraph(MapReduceState)\n",
    "    \n",
    "    # 添加节点\n",
    "    graph.add_node(\"split\", split_data)\n",
    "    graph.add_node(\"map_worker\", map_worker)\n",
    "    graph.add_node(\"reduce\", reduce_worker)\n",
    "    \n",
    "    # 设置流程\n",
    "    graph.set_entry_point(\"split\")\n",
    "    graph.add_conditional_edges(\"split\", create_map_tasks, [\"map_worker\"])\n",
    "    graph.add_edge(\"map_worker\", \"reduce\")\n",
    "    graph.add_edge(\"reduce\", END)\n",
    "    \n",
    "    return graph.compile()\n",
    "\n",
    "# 测试Map-Reduce\n",
    "mapreduce_app = create_mapreduce_graph()\n",
    "\n",
    "print(\"\\n=== Map-Reduce 演示 ===\")\n",
    "test_data = list(range(1, 21))  # 1到20的数字\n",
    "print(f\"输入数据: {test_data}\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "mapreduce_result = mapreduce_app.invoke({\n",
    "    \"input_data\": test_data,\n",
    "    \"chunk_size\": 5\n",
    "})\n",
    "\n",
    "execution_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n=== Map-Reduce 结果 ===\")\n",
    "print(f\"执行时间: {execution_time:.2f} 秒\")\n",
    "\n",
    "reduce_result = mapreduce_result.get(\"reduce_result\", {})\n",
    "print(f\"总平方和: {reduce_result.get('total_squared_sum', 0)}\")\n",
    "print(f\"全局最大值: {reduce_result.get('global_max', 0)}\")\n",
    "print(f\"全局最小值: {reduce_result.get('global_min', 0)}\")\n",
    "print(f\"平均平方值: {reduce_result.get('average_squared', 0):.2f}\")\n",
    "print(f\"处理总数: {reduce_result.get('total_count', 0)} 项\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 实践案例：大规模文本处理\n",
    "\n",
    "构建一个大规模文本处理系统："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文本处理状态\n",
    "class TextProcessingState(TypedDict):\n",
    "    documents: List[str]\n",
    "    processing_results: List[Dict[str, Any]]\n",
    "    final_statistics: Dict[str, Any]\n",
    "    batch_size: int\n",
    "\n",
    "def prepare_documents(state: TextProcessingState) -> TextProcessingState:\n",
    "    \"\"\"准备文档批次\"\"\"\n",
    "    # 模拟大量文档\n",
    "    documents = [\n",
    "        \"This is document 1. It contains some text for analysis.\",\n",
    "        \"Document 2 has different content and more words to process.\",\n",
    "        \"The third document discusses various topics and concepts.\",\n",
    "        \"Document 4 contains technical information about systems.\",\n",
    "        \"Fifth document explores advanced algorithmic approaches.\",\n",
    "        \"Document 6 covers machine learning and data science.\",\n",
    "        \"The seventh document examines distributed computing.\",\n",
    "        \"Document 8 focuses on parallel processing techniques.\",\n",
    "        \"Ninth document analyzes performance optimization methods.\",\n",
    "        \"The final document summarizes key findings and conclusions.\"\n",
    "    ]\n",
    "    \n",
    "    print(f\"📚 准备了 {len(documents)} 个文档进行处理\")\n",
    "    \n",
    "    return {\n",
    "        \"documents\": documents,\n",
    "        \"batch_size\": 3\n",
    "    }\n",
    "\n",
    "def create_processing_tasks(state: TextProcessingState) -> List[Send]:\n",
    "    \"\"\"创建文本处理任务\"\"\"\n",
    "    documents = state.get(\"documents\", [])\n",
    "    batch_size = state.get(\"batch_size\", 3)\n",
    "    \n",
    "    # 将文档分批\n",
    "    tasks = []\n",
    "    for i in range(0, len(documents), batch_size):\n",
    "        batch = documents[i:i + batch_size]\n",
    "        task = Send(\"process_batch\", {\n",
    "            \"batch_id\": i // batch_size,\n",
    "            \"batch_documents\": batch\n",
    "        })\n",
    "        tasks.append(task)\n",
    "    \n",
    "    print(f\"🚀 创建 {len(tasks)} 个处理批次\")\n",
    "    return tasks\n",
    "\n",
    "def process_batch(state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"处理文档批次\"\"\"\n",
    "    batch_id = state[\"batch_id\"]\n",
    "    documents = state[\"batch_documents\"]\n",
    "    \n",
    "    print(f\"📄 批次 {batch_id}: 处理 {len(documents)} 个文档\")\n",
    "    \n",
    "    # 模拟文本分析\n",
    "    batch_stats = {\n",
    "        \"batch_id\": batch_id,\n",
    "        \"document_count\": len(documents),\n",
    "        \"total_words\": 0,\n",
    "        \"total_characters\": 0,\n",
    "        \"word_frequencies\": {},\n",
    "        \"avg_doc_length\": 0\n",
    "    }\n",
    "    \n",
    "    all_words = []\n",
    "    total_chars = 0\n",
    "    \n",
    "    for doc in documents:\n",
    "        # 简单的文本处理\n",
    "        words = doc.lower().replace('.', '').replace(',', '').split()\n",
    "        all_words.extend(words)\n",
    "        total_chars += len(doc)\n",
    "    \n",
    "    # 计算统计信息\n",
    "    word_freq = {}\n",
    "    for word in all_words:\n",
    "        word_freq[word] = word_freq.get(word, 0) + 1\n",
    "    \n",
    "    batch_stats.update({\n",
    "        \"total_words\": len(all_words),\n",
    "        \"total_characters\": total_chars,\n",
    "        \"word_frequencies\": dict(list(word_freq.items())[:10]),  # 只保留前10个\n",
    "        \"avg_doc_length\": total_chars / len(documents) if documents else 0\n",
    "    })\n",
    "    \n",
    "    # 模拟处理时间\n",
    "    time.sleep(0.8)\n",
    "    \n",
    "    print(f\"✅ 批次 {batch_id} 完成: {len(all_words)} 个单词, {total_chars} 个字符\")\n",
    "    \n",
    "    return {\n",
    "        \"processing_results\": [batch_stats]\n",
    "    }\n",
    "\n",
    "def aggregate_statistics(state: TextProcessingState) -> TextProcessingState:\n",
    "    \"\"\"聚合统计信息\"\"\"\n",
    "    results = state.get(\"processing_results\", [])\n",
    "    \n",
    "    print(f\"📊 聚合 {len(results)} 个批次的结果\")\n",
    "    \n",
    "    # 合并所有统计信息\n",
    "    total_docs = sum(r[\"document_count\"] for r in results)\n",
    "    total_words = sum(r[\"total_words\"] for r in results)\n",
    "    total_chars = sum(r[\"total_characters\"] for r in results)\n",
    "    \n",
    "    # 合并词频\n",
    "    combined_freq = {}\n",
    "    for result in results:\n",
    "        for word, freq in result[\"word_frequencies\"].items():\n",
    "            combined_freq[word] = combined_freq.get(word, 0) + freq\n",
    "    \n",
    "    # 获取最高频词汇\n",
    "    top_words = sorted(combined_freq.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    \n",
    "    final_stats = {\n",
    "        \"total_documents\": total_docs,\n",
    "        \"total_words\": total_words,\n",
    "        \"total_characters\": total_chars,\n",
    "        \"avg_words_per_doc\": total_words / total_docs if total_docs > 0 else 0,\n",
    "        \"avg_chars_per_doc\": total_chars / total_docs if total_docs > 0 else 0,\n",
    "        \"top_10_words\": top_words,\n",
    "        \"processing_batches\": len(results)\n",
    "    }\n",
    "    \n",
    "    print(f\"✅ 聚合完成: {total_docs} 文档, {total_words} 单词\")\n",
    "    \n",
    "    return {\"final_statistics\": final_stats}\n",
    "\n",
    "# 创建文本处理图\n",
    "def create_text_processing_graph():\n",
    "    graph = StateGraph(TextProcessingState)\n",
    "    \n",
    "    graph.add_node(\"prepare\", prepare_documents)\n",
    "    graph.add_node(\"process_batch\", process_batch)\n",
    "    graph.add_node(\"aggregate\", aggregate_statistics)\n",
    "    \n",
    "    graph.set_entry_point(\"prepare\")\n",
    "    graph.add_conditional_edges(\"prepare\", create_processing_tasks, [\"process_batch\"])\n",
    "    graph.add_edge(\"process_batch\", \"aggregate\")\n",
    "    graph.add_edge(\"aggregate\", END)\n",
    "    \n",
    "    return graph.compile()\n",
    "\n",
    "# 测试文本处理系统\n",
    "text_app = create_text_processing_graph()\n",
    "\n",
    "print(\"\\n=== 大规模文本处理演示 ===\")\n",
    "start_time = time.time()\n",
    "\n",
    "text_result = text_app.invoke({})\n",
    "\n",
    "execution_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n=== 文本处理结果 ===\")\n",
    "print(f\"总执行时间: {execution_time:.2f} 秒\")\n",
    "\n",
    "stats = text_result.get(\"final_statistics\", {})\n",
    "print(f\"处理文档数: {stats.get('total_documents', 0)}\")\n",
    "print(f\"总单词数: {stats.get('total_words', 0)}\")\n",
    "print(f\"总字符数: {stats.get('total_characters', 0)}\")\n",
    "print(f\"平均每文档单词数: {stats.get('avg_words_per_doc', 0):.1f}\")\n",
    "print(f\"处理批次数: {stats.get('processing_batches', 0)}\")\n",
    "\n",
    "print(\"\\n高频词汇:\")\n",
    "for word, freq in stats.get('top_10_words', [])[:5]:\n",
    "    print(f\"  {word}: {freq} 次\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 练习题\n",
    "\n",
    "### 练习1：并行图像处理系统\n",
    "创建一个并行图像处理系统，支持多种滤镜效果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 练习1：请实现并行图像处理系统\n",
    "# TODO: 创建多个图像滤镜节点\n",
    "# TODO: 实现并行处理多张图片\n",
    "# TODO: 支持不同的处理效果组合\n",
    "\n",
    "print(\"请实现并行图像处理系统\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习2：分布式数据分析\n",
    "构建一个分布式数据分析系统："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 练习2：请实现分布式数据分析系统\n",
    "# TODO: 实现数据分片和分发\n",
    "# TODO: 并行统计分析\n",
    "# TODO: 结果聚合和报告生成\n",
    "\n",
    "print(\"请实现分布式数据分析系统\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "在本课中，我们学习了并行执行和Map-Reduce模式：\n",
    "\n",
    "### 关键要点：\n",
    "1. **并行执行**：同时运行多个节点提高效率\n",
    "2. **Map-Reduce**：分而治之的处理模式\n",
    "3. **Send API**：动态创建并行任务\n",
    "4. **结果聚合**：合并并行处理结果\n",
    "5. **性能优化**：合理设计并行度和批次大小\n",
    "\n",
    "### 最佳实践：\n",
    "- **合理分片**：平衡负载和通信开销\n",
    "- **错误处理**：处理并行任务中的异常\n",
    "- **资源管理**：控制并发度避免资源耗尽\n",
    "- **监控调试**：跟踪并行任务执行状态\n",
    "\n",
    "### 应用场景：\n",
    "- 大数据处理\n",
    "- 图像/视频处理\n",
    "- 科学计算\n",
    "- 机器学习训练\n",
    "- 爬虫和数据采集\n",
    "\n",
    "## 下一课预告\n",
    "\n",
    "在下一课《Human-in-the-Loop》中，我们将学习：\n",
    "- 人机交互节点设计\n",
    "- 中断机制实现\n",
    "- 人工审批流程\n",
    "- 输入验证和确认\n",
    "- 交互式对话系统构建"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}