{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05. å­å›¾å’ŒåµŒå¥—å›¾\n",
    "\n",
    "## è¯¾ç¨‹ç›®æ ‡\n",
    "- ç†è§£å­å›¾ï¼ˆSubgraphsï¼‰çš„æ¦‚å¿µå’Œä¼˜åŠ¿\n",
    "- å­¦ä¹ åµŒå¥—å›¾ç»“æ„çš„è®¾è®¡\n",
    "- æŒæ¡å­å›¾çš„ç¼–è¯‘å’Œæ‰§è¡Œ\n",
    "- ç†è§£çŠ¶æ€ä¼ é€’å’Œéš”ç¦»æœºåˆ¶\n",
    "- å®ç°æ¨¡å—åŒ–è®¾è®¡æ¨¡å¼\n",
    "\n",
    "## æ ¸å¿ƒæ¦‚å¿µ\n",
    "å­å›¾æ˜¯LangGraphä¸­ç”¨äºæ„å»ºå¤æ‚åº”ç”¨çš„é‡è¦å·¥å…·ï¼š\n",
    "1. **æ¨¡å—åŒ–è®¾è®¡**ï¼šå°†å¤æ‚ç³»ç»Ÿåˆ†è§£ä¸ºç‹¬ç«‹çš„å­ç³»ç»Ÿ\n",
    "2. **çŠ¶æ€éš”ç¦»**ï¼šæ¯ä¸ªå­å›¾å¯ä»¥æœ‰ç‹¬ç«‹çš„çŠ¶æ€ç®¡ç†\n",
    "3. **å¯é‡ç”¨æ€§**ï¼šå­å›¾å¯ä»¥åœ¨å¤šä¸ªåœ°æ–¹é‡ç”¨\n",
    "4. **å±‚æ¬¡ç»“æ„**ï¼šæ”¯æŒå¤šå±‚åµŒå¥—çš„å›¾ç»“æ„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç¯å¢ƒå‡†å¤‡\n",
    "from typing import TypedDict, Annotated, List, Dict, Any\n",
    "from langgraph.graph import StateGraph, END\n",
    "import json\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"ç¯å¢ƒå‡†å¤‡å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. åŸºç¡€å­å›¾ç¤ºä¾‹\n",
    "\n",
    "è®©æˆ‘ä»¬ä»ä¸€ä¸ªç®€å•çš„å­å›¾å¼€å§‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®šä¹‰å­å›¾çŠ¶æ€\n",
    "class SubGraphState(TypedDict):\n",
    "    input_data: str\n",
    "    processed_data: str\n",
    "    step_count: int\n",
    "    logs: List[str]\n",
    "\n",
    "# å­å›¾èŠ‚ç‚¹å‡½æ•°\n",
    "def preprocess_data(state: SubGraphState) -> SubGraphState:\n",
    "    \"\"\"é¢„å¤„ç†æ•°æ®\"\"\"\n",
    "    input_data = state.get(\"input_data\", \"\")\n",
    "    processed = f\"preprocessed_{input_data}\"\n",
    "    \n",
    "    print(f\"ğŸ“ é¢„å¤„ç†æ•°æ®: {input_data} -> {processed}\")\n",
    "    \n",
    "    return {\n",
    "        \"processed_data\": processed,\n",
    "        \"step_count\": state.get(\"step_count\", 0) + 1,\n",
    "        \"logs\": state.get(\"logs\", []) + [f\"é¢„å¤„ç†å®Œæˆ: {processed}\"]\n",
    "    }\n",
    "\n",
    "def validate_data(state: SubGraphState) -> SubGraphState:\n",
    "    \"\"\"éªŒè¯æ•°æ®\"\"\"\n",
    "    processed_data = state.get(\"processed_data\", \"\")\n",
    "    is_valid = len(processed_data) > 5  # ç®€å•éªŒè¯é€»è¾‘\n",
    "    \n",
    "    status = \"æœ‰æ•ˆ\" if is_valid else \"æ— æ•ˆ\"\n",
    "    print(f\"âœ… æ•°æ®éªŒè¯: {processed_data} -> {status}\")\n",
    "    \n",
    "    return {\n",
    "        \"step_count\": state.get(\"step_count\", 0) + 1,\n",
    "        \"logs\": state.get(\"logs\", []) + [f\"éªŒè¯ç»“æœ: {status}\"]\n",
    "    }\n",
    "\n",
    "def transform_data(state: SubGraphState) -> SubGraphState:\n",
    "    \"\"\"è½¬æ¢æ•°æ®\"\"\"\n",
    "    processed_data = state.get(\"processed_data\", \"\")\n",
    "    transformed = f\"transformed_{processed_data.upper()}\"\n",
    "    \n",
    "    print(f\"ğŸ”„ è½¬æ¢æ•°æ®: {processed_data} -> {transformed}\")\n",
    "    \n",
    "    return {\n",
    "        \"processed_data\": transformed,\n",
    "        \"step_count\": state.get(\"step_count\", 0) + 1,\n",
    "        \"logs\": state.get(\"logs\", []) + [f\"è½¬æ¢å®Œæˆ: {transformed}\"]\n",
    "    }\n",
    "\n",
    "# åˆ›å»ºå­å›¾\n",
    "def create_data_processing_subgraph():\n",
    "    subgraph = StateGraph(SubGraphState)\n",
    "    \n",
    "    # æ·»åŠ èŠ‚ç‚¹\n",
    "    subgraph.add_node(\"preprocess\", preprocess_data)\n",
    "    subgraph.add_node(\"validate\", validate_data)\n",
    "    subgraph.add_node(\"transform\", transform_data)\n",
    "    \n",
    "    # è®¾ç½®æµç¨‹\n",
    "    subgraph.set_entry_point(\"preprocess\")\n",
    "    subgraph.add_edge(\"preprocess\", \"validate\")\n",
    "    subgraph.add_edge(\"validate\", \"transform\")\n",
    "    subgraph.add_edge(\"transform\", END)\n",
    "    \n",
    "    return subgraph.compile()\n",
    "\n",
    "# æµ‹è¯•å­å›¾\n",
    "data_subgraph = create_data_processing_subgraph()\n",
    "\n",
    "# æ‰§è¡Œå­å›¾\n",
    "test_input = \"sample_data\"\n",
    "result = data_subgraph.invoke({\n",
    "    \"input_data\": test_input,\n",
    "    \"step_count\": 0,\n",
    "    \"logs\": []\n",
    "})\n",
    "\n",
    "print(\"\\nå­å›¾æ‰§è¡Œç»“æœ:\")\n",
    "print(json.dumps(result, ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. åµŒå¥—å›¾ç»“æ„\n",
    "\n",
    "åˆ›å»ºåŒ…å«å­å›¾çš„ä¸»å›¾ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¸»å›¾çŠ¶æ€\n",
    "class MainGraphState(TypedDict):\n",
    "    raw_input: str\n",
    "    batch_data: List[str]\n",
    "    processing_results: List[Dict[str, Any]]\n",
    "    final_output: str\n",
    "    total_steps: int\n",
    "\n",
    "# ä¸»å›¾èŠ‚ç‚¹å‡½æ•°\n",
    "def prepare_batch(state: MainGraphState) -> MainGraphState:\n",
    "    \"\"\"å‡†å¤‡æ‰¹é‡æ•°æ®\"\"\"\n",
    "    raw_input = state.get(\"raw_input\", \"\")\n",
    "    \n",
    "    # æ¨¡æ‹Ÿå°†è¾“å…¥åˆ†æ‰¹å¤„ç†\n",
    "    batch_data = [f\"{raw_input}_batch_{i}\" for i in range(3)]\n",
    "    \n",
    "    print(f\"ğŸ“¦ å‡†å¤‡æ‰¹é‡æ•°æ®: {batch_data}\")\n",
    "    \n",
    "    return {\n",
    "        \"batch_data\": batch_data,\n",
    "        \"processing_results\": [],\n",
    "        \"total_steps\": 1\n",
    "    }\n",
    "\n",
    "def process_with_subgraph(state: MainGraphState) -> MainGraphState:\n",
    "    \"\"\"ä½¿ç”¨å­å›¾å¤„ç†æ¯æ‰¹æ•°æ®\"\"\"\n",
    "    batch_data = state.get(\"batch_data\", [])\n",
    "    processing_results = []\n",
    "    \n",
    "    print(\"ğŸ”„ å¼€å§‹æ‰¹é‡å¤„ç†...\")\n",
    "    \n",
    "    # ä¸ºæ¯æ‰¹æ•°æ®è°ƒç”¨å­å›¾\n",
    "    for i, data in enumerate(batch_data):\n",
    "        print(f\"\\n--- å¤„ç†æ‰¹æ¬¡ {i+1}: {data} ---\")\n",
    "        \n",
    "        # è°ƒç”¨å­å›¾\n",
    "        subgraph_result = data_subgraph.invoke({\n",
    "            \"input_data\": data,\n",
    "            \"step_count\": 0,\n",
    "            \"logs\": []\n",
    "        })\n",
    "        \n",
    "        processing_results.append({\n",
    "            \"batch_id\": i,\n",
    "            \"input\": data,\n",
    "            \"output\": subgraph_result.get(\"processed_data\", \"\"),\n",
    "            \"steps\": subgraph_result.get(\"step_count\", 0),\n",
    "            \"logs\": subgraph_result.get(\"logs\", [])\n",
    "        })\n",
    "    \n",
    "    return {\n",
    "        \"processing_results\": processing_results,\n",
    "        \"total_steps\": state.get(\"total_steps\", 0) + 1\n",
    "    }\n",
    "\n",
    "def aggregate_results(state: MainGraphState) -> MainGraphState:\n",
    "    \"\"\"èšåˆå¤„ç†ç»“æœ\"\"\"\n",
    "    processing_results = state.get(\"processing_results\", [])\n",
    "    \n",
    "    # èšåˆæ‰€æœ‰å¤„ç†ç»“æœ\n",
    "    all_outputs = [result[\"output\"] for result in processing_results]\n",
    "    final_output = \" | \".join(all_outputs)\n",
    "    \n",
    "    total_sub_steps = sum(result[\"steps\"] for result in processing_results)\n",
    "    \n",
    "    print(f\"ğŸ“Š èšåˆç»“æœ: {final_output}\")\n",
    "    print(f\"ğŸ“ˆ æ€»å­æ­¥éª¤æ•°: {total_sub_steps}\")\n",
    "    \n",
    "    return {\n",
    "        \"final_output\": final_output,\n",
    "        \"total_steps\": state.get(\"total_steps\", 0) + 1\n",
    "    }\n",
    "\n",
    "# åˆ›å»ºä¸»å›¾\n",
    "def create_main_graph():\n",
    "    main_graph = StateGraph(MainGraphState)\n",
    "    \n",
    "    # æ·»åŠ èŠ‚ç‚¹\n",
    "    main_graph.add_node(\"prepare\", prepare_batch)\n",
    "    main_graph.add_node(\"process\", process_with_subgraph)\n",
    "    main_graph.add_node(\"aggregate\", aggregate_results)\n",
    "    \n",
    "    # è®¾ç½®æµç¨‹\n",
    "    main_graph.set_entry_point(\"prepare\")\n",
    "    main_graph.add_edge(\"prepare\", \"process\")\n",
    "    main_graph.add_edge(\"process\", \"aggregate\")\n",
    "    main_graph.add_edge(\"aggregate\", END)\n",
    "    \n",
    "    return main_graph.compile()\n",
    "\n",
    "# æµ‹è¯•åµŒå¥—å›¾\n",
    "main_graph = create_main_graph()\n",
    "\n",
    "print(\"\\n=== åµŒå¥—å›¾æ‰§è¡Œæµ‹è¯• ===\")\n",
    "nested_result = main_graph.invoke({\n",
    "    \"raw_input\": \"user_data\"\n",
    "})\n",
    "\n",
    "print(\"\\nåµŒå¥—å›¾æœ€ç»ˆç»“æœ:\")\n",
    "print(f\"è¾“å…¥: {nested_result.get('raw_input', 'N/A')}\")\n",
    "print(f\"æ‰¹é‡æ•°æ®: {nested_result.get('batch_data', [])}\")\n",
    "print(f\"æœ€ç»ˆè¾“å‡º: {nested_result.get('final_output', 'N/A')}\")\n",
    "print(f\"ä¸»å›¾æ€»æ­¥éª¤: {nested_result.get('total_steps', 0)}\")\n",
    "\n",
    "# æ˜¾ç¤ºæ¯ä¸ªæ‰¹æ¬¡çš„è¯¦ç»†ç»“æœ\n",
    "print(\"\\nå„æ‰¹æ¬¡å¤„ç†è¯¦æƒ…:\")\n",
    "for result in nested_result.get('processing_results', []):\n",
    "    print(f\"æ‰¹æ¬¡ {result['batch_id']}: {result['input']} -> {result['output']} ({result['steps']} æ­¥éª¤)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. çŠ¶æ€ä¼ é€’å’Œéš”ç¦»\n",
    "\n",
    "æ¼”ç¤ºä¸åŒå›¾ä¹‹é—´çš„çŠ¶æ€ç®¡ç†ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®šä¹‰ä¸åŒçš„çŠ¶æ€ç±»å‹\n",
    "class ParentState(TypedDict):\n",
    "    user_id: str\n",
    "    session_data: Dict[str, Any]\n",
    "    child_results: List[Dict[str, Any]]\n",
    "    global_config: Dict[str, Any]\n",
    "\n",
    "class ChildState(TypedDict):\n",
    "    task_id: str\n",
    "    local_data: str\n",
    "    processing_status: str\n",
    "    internal_state: Dict[str, Any]\n",
    "    inherited_config: Dict[str, Any]\n",
    "\n",
    "# å­å›¾ï¼šä»»åŠ¡å¤„ç†å™¨\n",
    "def initialize_task(state: ChildState) -> ChildState:\n",
    "    \"\"\"åˆå§‹åŒ–ä»»åŠ¡\"\"\"\n",
    "    task_id = state.get(\"task_id\", f\"task_{random.randint(1000, 9999)}\")\n",
    "    config = state.get(\"inherited_config\", {})\n",
    "    \n",
    "    print(f\"ğŸš€ åˆå§‹åŒ–ä»»åŠ¡: {task_id}\")\n",
    "    print(f\"ğŸ“‹ ç»§æ‰¿é…ç½®: {config}\")\n",
    "    \n",
    "    return {\n",
    "        \"task_id\": task_id,\n",
    "        \"processing_status\": \"initialized\",\n",
    "        \"internal_state\": {\n",
    "            \"start_time\": datetime.now().isoformat(),\n",
    "            \"steps_completed\": 0\n",
    "        }\n",
    "    }\n",
    "\n",
    "def execute_task(state: ChildState) -> ChildState:\n",
    "    \"\"\"æ‰§è¡Œä»»åŠ¡\"\"\"\n",
    "    task_id = state.get(\"task_id\", \"\")\n",
    "    local_data = state.get(\"local_data\", \"\")\n",
    "    internal_state = state.get(\"internal_state\", {})\n",
    "    \n",
    "    print(f\"âš™ï¸ æ‰§è¡Œä»»åŠ¡: {task_id} å¤„ç†æ•°æ®: {local_data}\")\n",
    "    \n",
    "    # æ¨¡æ‹Ÿä»»åŠ¡å¤„ç†\n",
    "    processed_data = f\"processed_{local_data}\"\n",
    "    \n",
    "    updated_internal = {\n",
    "        **internal_state,\n",
    "        \"steps_completed\": internal_state.get(\"steps_completed\", 0) + 1,\n",
    "        \"last_processed\": processed_data\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        \"local_data\": processed_data,\n",
    "        \"processing_status\": \"completed\",\n",
    "        \"internal_state\": updated_internal\n",
    "    }\n",
    "\n",
    "def finalize_task(state: ChildState) -> ChildState:\n",
    "    \"\"\"å®Œæˆä»»åŠ¡\"\"\"\n",
    "    task_id = state.get(\"task_id\", \"\")\n",
    "    internal_state = state.get(\"internal_state\", {})\n",
    "    \n",
    "    print(f\"âœ… å®Œæˆä»»åŠ¡: {task_id}\")\n",
    "    \n",
    "    final_internal = {\n",
    "        **internal_state,\n",
    "        \"end_time\": datetime.now().isoformat(),\n",
    "        \"status\": \"finalized\"\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        \"processing_status\": \"finalized\",\n",
    "        \"internal_state\": final_internal\n",
    "    }\n",
    "\n",
    "# åˆ›å»ºå­å›¾\n",
    "def create_task_subgraph():\n",
    "    task_graph = StateGraph(ChildState)\n",
    "    \n",
    "    task_graph.add_node(\"init\", initialize_task)\n",
    "    task_graph.add_node(\"execute\", execute_task)\n",
    "    task_graph.add_node(\"finalize\", finalize_task)\n",
    "    \n",
    "    task_graph.set_entry_point(\"init\")\n",
    "    task_graph.add_edge(\"init\", \"execute\")\n",
    "    task_graph.add_edge(\"execute\", \"finalize\")\n",
    "    task_graph.add_edge(\"finalize\", END)\n",
    "    \n",
    "    return task_graph.compile()\n",
    "\n",
    "# çˆ¶å›¾èŠ‚ç‚¹\n",
    "def setup_session(state: ParentState) -> ParentState:\n",
    "    \"\"\"è®¾ç½®ä¼šè¯\"\"\"\n",
    "    user_id = state.get(\"user_id\", f\"user_{random.randint(100, 999)}\")\n",
    "    \n",
    "    session_data = {\n",
    "        \"session_id\": f\"session_{random.randint(1000, 9999)}\",\n",
    "        \"start_time\": datetime.now().isoformat(),\n",
    "        \"tasks_created\": 0\n",
    "    }\n",
    "    \n",
    "    global_config = {\n",
    "        \"timeout\": 30,\n",
    "        \"retry_count\": 3,\n",
    "        \"debug_mode\": True\n",
    "    }\n",
    "    \n",
    "    print(f\"ğŸ‘¤ è®¾ç½®ç”¨æˆ·ä¼šè¯: {user_id}\")\n",
    "    print(f\"ğŸ”§ å…¨å±€é…ç½®: {global_config}\")\n",
    "    \n",
    "    return {\n",
    "        \"user_id\": user_id,\n",
    "        \"session_data\": session_data,\n",
    "        \"global_config\": global_config,\n",
    "        \"child_results\": []\n",
    "    }\n",
    "\n",
    "def orchestrate_tasks(state: ParentState) -> ParentState:\n",
    "    \"\"\"ç¼–æ’ä»»åŠ¡æ‰§è¡Œ\"\"\"\n",
    "    session_data = state.get(\"session_data\", {})\n",
    "    global_config = state.get(\"global_config\", {})\n",
    "    \n",
    "    # åˆ›å»ºå­å›¾å®ä¾‹\n",
    "    task_subgraph = create_task_subgraph()\n",
    "    \n",
    "    # æ¨¡æ‹Ÿåˆ›å»ºå¤šä¸ªä»»åŠ¡\n",
    "    tasks_to_create = [\"task_a\", \"task_b\", \"task_c\"]\n",
    "    child_results = []\n",
    "    \n",
    "    print(\"\\nğŸ¯ å¼€å§‹ç¼–æ’ä»»åŠ¡æ‰§è¡Œ...\")\n",
    "    \n",
    "    for i, task_data in enumerate(tasks_to_create):\n",
    "        print(f\"\\n--- æ‰§è¡Œå­ä»»åŠ¡ {i+1}: {task_data} ---\")\n",
    "        \n",
    "        # å‡†å¤‡å­å›¾è¾“å…¥ï¼ˆçŠ¶æ€éš”ç¦»ï¼‰\n",
    "        child_input = {\n",
    "            \"task_id\": f\"{task_data}_{i}\",\n",
    "            \"local_data\": task_data,\n",
    "            \"processing_status\": \"pending\",\n",
    "            \"internal_state\": {},\n",
    "            \"inherited_config\": global_config  # ä¼ é€’å…¨å±€é…ç½®\n",
    "        }\n",
    "        \n",
    "        # æ‰§è¡Œå­å›¾\n",
    "        child_result = task_subgraph.invoke(child_input)\n",
    "        \n",
    "        # æ”¶é›†ç»“æœï¼ˆåªä¿ç•™å¿…è¦ä¿¡æ¯ï¼‰\n",
    "        child_results.append({\n",
    "            \"task_id\": child_result.get(\"task_id\"),\n",
    "            \"status\": child_result.get(\"processing_status\"),\n",
    "            \"output_data\": child_result.get(\"local_data\"),\n",
    "            \"execution_summary\": {\n",
    "                \"steps\": child_result.get(\"internal_state\", {}).get(\"steps_completed\", 0),\n",
    "                \"duration\": \"calculated_duration\"  # å®é™…åº”ç”¨ä¸­è®¡ç®—æ—¶é—´å·®\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    # æ›´æ–°ä¼šè¯æ•°æ®\n",
    "    updated_session = {\n",
    "        **session_data,\n",
    "        \"tasks_created\": len(tasks_to_create),\n",
    "        \"tasks_completed\": len([r for r in child_results if r[\"status\"] == \"finalized\"])\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        \"session_data\": updated_session,\n",
    "        \"child_results\": child_results\n",
    "    }\n",
    "\n",
    "def generate_report(state: ParentState) -> ParentState:\n",
    "    \"\"\"ç”ŸæˆæŠ¥å‘Š\"\"\"\n",
    "    user_id = state.get(\"user_id\", \"\")\n",
    "    session_data = state.get(\"session_data\", {})\n",
    "    child_results = state.get(\"child_results\", [])\n",
    "    \n",
    "    print(f\"\\nğŸ“Š ç”Ÿæˆæ‰§è¡ŒæŠ¥å‘Š...\")\n",
    "    print(f\"ç”¨æˆ·: {user_id}\")\n",
    "    print(f\"ä¼šè¯: {session_data.get('session_id', 'N/A')}\")\n",
    "    print(f\"åˆ›å»ºä»»åŠ¡æ•°: {session_data.get('tasks_created', 0)}\")\n",
    "    print(f\"å®Œæˆä»»åŠ¡æ•°: {session_data.get('tasks_completed', 0)}\")\n",
    "    \n",
    "    print(\"\\nä»»åŠ¡æ‰§è¡Œè¯¦æƒ…:\")\n",
    "    for result in child_results:\n",
    "        print(f\"- {result['task_id']}: {result['status']} -> {result['output_data']}\")\n",
    "    \n",
    "    return state\n",
    "\n",
    "# åˆ›å»ºçˆ¶å›¾\n",
    "def create_parent_graph():\n",
    "    parent_graph = StateGraph(ParentState)\n",
    "    \n",
    "    parent_graph.add_node(\"setup\", setup_session)\n",
    "    parent_graph.add_node(\"orchestrate\", orchestrate_tasks)\n",
    "    parent_graph.add_node(\"report\", generate_report)\n",
    "    \n",
    "    parent_graph.set_entry_point(\"setup\")\n",
    "    parent_graph.add_edge(\"setup\", \"orchestrate\")\n",
    "    parent_graph.add_edge(\"orchestrate\", \"report\")\n",
    "    parent_graph.add_edge(\"report\", END)\n",
    "    \n",
    "    return parent_graph.compile()\n",
    "\n",
    "# æµ‹è¯•çŠ¶æ€éš”ç¦»\n",
    "print(\"\\n=== çŠ¶æ€éš”ç¦»æ¼”ç¤º ===\")\n",
    "parent_graph = create_parent_graph()\n",
    "\n",
    "isolation_result = parent_graph.invoke({\n",
    "    \"user_id\": \"demo_user\"\n",
    "})\n",
    "\n",
    "print(\"\\n=== æœ€ç»ˆçŠ¶æ€æ‘˜è¦ ===\")\n",
    "print(f\"ç”¨æˆ·ID: {isolation_result.get('user_id')}\")\n",
    "print(f\"ä¼šè¯ä¿¡æ¯: {isolation_result.get('session_data')}\")\n",
    "print(f\"å­ä»»åŠ¡ç»“æœæ•°é‡: {len(isolation_result.get('child_results', []))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. å®è·µæ¡ˆä¾‹ï¼šå¤šé˜¶æ®µæ•°æ®åˆ†æç³»ç»Ÿ\n",
    "\n",
    "æ„å»ºä¸€ä¸ªåŒ…å«å¤šä¸ªä¸“é—¨å­å›¾çš„å¤æ‚åˆ†æç³»ç»Ÿï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ•°æ®æ¸…ç†å­å›¾\n",
    "class CleaningState(TypedDict):\n",
    "    raw_data: List[str]\n",
    "    cleaned_data: List[str]\n",
    "    cleaning_report: Dict[str, Any]\n",
    "\n",
    "def remove_duplicates(state: CleaningState) -> CleaningState:\n",
    "    raw_data = state.get(\"raw_data\", [])\n",
    "    cleaned_data = list(set(raw_data))  # å»é‡\n",
    "    removed_count = len(raw_data) - len(cleaned_data)\n",
    "    \n",
    "    print(f\"ğŸ§¹ å»é‡å¤„ç†: ç§»é™¤äº† {removed_count} ä¸ªé‡å¤é¡¹\")\n",
    "    \n",
    "    return {\n",
    "        \"cleaned_data\": cleaned_data,\n",
    "        \"cleaning_report\": {\"duplicates_removed\": removed_count}\n",
    "    }\n",
    "\n",
    "def filter_invalid(state: CleaningState) -> CleaningState:\n",
    "    cleaned_data = state.get(\"cleaned_data\", [])\n",
    "    valid_data = [item for item in cleaned_data if len(item) >= 3]  # ç®€å•è¿‡æ»¤\n",
    "    filtered_count = len(cleaned_data) - len(valid_data)\n",
    "    \n",
    "    print(f\"ğŸ” è¿‡æ»¤æ— æ•ˆæ•°æ®: è¿‡æ»¤äº† {filtered_count} ä¸ªæ— æ•ˆé¡¹\")\n",
    "    \n",
    "    report = state.get(\"cleaning_report\", {})\n",
    "    report[\"invalid_filtered\"] = filtered_count\n",
    "    \n",
    "    return {\n",
    "        \"cleaned_data\": valid_data,\n",
    "        \"cleaning_report\": report\n",
    "    }\n",
    "\n",
    "# åˆ›å»ºæ•°æ®æ¸…ç†å­å›¾\n",
    "def create_cleaning_subgraph():\n",
    "    cleaning_graph = StateGraph(CleaningState)\n",
    "    cleaning_graph.add_node(\"deduplicate\", remove_duplicates)\n",
    "    cleaning_graph.add_node(\"filter\", filter_invalid)\n",
    "    \n",
    "    cleaning_graph.set_entry_point(\"deduplicate\")\n",
    "    cleaning_graph.add_edge(\"deduplicate\", \"filter\")\n",
    "    cleaning_graph.add_edge(\"filter\", END)\n",
    "    \n",
    "    return cleaning_graph.compile()\n",
    "\n",
    "# åˆ†æå­å›¾\n",
    "class AnalysisState(TypedDict):\n",
    "    clean_data: List[str]\n",
    "    statistics: Dict[str, Any]\n",
    "    insights: List[str]\n",
    "\n",
    "def calculate_stats(state: AnalysisState) -> AnalysisState:\n",
    "    clean_data = state.get(\"clean_data\", [])\n",
    "    \n",
    "    stats = {\n",
    "        \"total_items\": len(clean_data),\n",
    "        \"avg_length\": sum(len(item) for item in clean_data) / len(clean_data) if clean_data else 0,\n",
    "        \"unique_chars\": len(set(''.join(clean_data))),\n",
    "        \"longest_item\": max(clean_data, key=len) if clean_data else \"\"\n",
    "    }\n",
    "    \n",
    "    print(f\"ğŸ“Š è®¡ç®—ç»Ÿè®¡ä¿¡æ¯: {stats}\")\n",
    "    \n",
    "    return {\"statistics\": stats}\n",
    "\n",
    "def generate_insights(state: AnalysisState) -> AnalysisState:\n",
    "    stats = state.get(\"statistics\", {})\n",
    "    insights = []\n",
    "    \n",
    "    if stats.get(\"total_items\", 0) > 10:\n",
    "        insights.append(\"æ•°æ®é›†è§„æ¨¡è¾ƒå¤§\")\n",
    "    \n",
    "    if stats.get(\"avg_length\", 0) > 5:\n",
    "        insights.append(\"æ•°æ®é¡¹å¹³å‡é•¿åº¦è¾ƒé•¿\")\n",
    "    \n",
    "    if stats.get(\"unique_chars\", 0) > 20:\n",
    "        insights.append(\"å­—ç¬¦å¤šæ ·æ€§é«˜\")\n",
    "    \n",
    "    print(f\"ğŸ’¡ ç”Ÿæˆæ´å¯Ÿ: {insights}\")\n",
    "    \n",
    "    return {\"insights\": insights}\n",
    "\n",
    "# åˆ›å»ºåˆ†æå­å›¾\n",
    "def create_analysis_subgraph():\n",
    "    analysis_graph = StateGraph(AnalysisState)\n",
    "    analysis_graph.add_node(\"stats\", calculate_stats)\n",
    "    analysis_graph.add_node(\"insights\", generate_insights)\n",
    "    \n",
    "    analysis_graph.set_entry_point(\"stats\")\n",
    "    analysis_graph.add_edge(\"stats\", \"insights\")\n",
    "    analysis_graph.add_edge(\"insights\", END)\n",
    "    \n",
    "    return analysis_graph.compile()\n",
    "\n",
    "# ä¸»åˆ†æç³»ç»ŸçŠ¶æ€\n",
    "class AnalysisSystemState(TypedDict):\n",
    "    input_data: List[str]\n",
    "    cleaning_results: Dict[str, Any]\n",
    "    analysis_results: Dict[str, Any]\n",
    "    final_report: Dict[str, Any]\n",
    "\n",
    "# ä¸»ç³»ç»ŸèŠ‚ç‚¹\n",
    "def load_data(state: AnalysisSystemState) -> AnalysisSystemState:\n",
    "    # æ¨¡æ‹ŸåŠ è½½æ•°æ®\n",
    "    input_data = [\n",
    "        \"data1\", \"data2\", \"data1\", \"abc\", \"xyz\", \"data3\", \"ab\", \"data4\", \"xyz\", \"data5\",\n",
    "        \"long_data_item\", \"another_long_item\", \"short\", \"x\", \"medium_data\"\n",
    "    ]\n",
    "    \n",
    "    print(f\"ğŸ“ åŠ è½½æ•°æ®: {len(input_data)} é¡¹\")\n",
    "    return {\"input_data\": input_data}\n",
    "\n",
    "def execute_cleaning(state: AnalysisSystemState) -> AnalysisSystemState:\n",
    "    input_data = state.get(\"input_data\", [])\n",
    "    \n",
    "    # è°ƒç”¨æ¸…ç†å­å›¾\n",
    "    cleaning_subgraph = create_cleaning_subgraph()\n",
    "    cleaning_result = cleaning_subgraph.invoke({\n",
    "        \"raw_data\": input_data\n",
    "    })\n",
    "    \n",
    "    return {\"cleaning_results\": cleaning_result}\n",
    "\n",
    "def execute_analysis(state: AnalysisSystemState) -> AnalysisSystemState:\n",
    "    cleaning_results = state.get(\"cleaning_results\", {})\n",
    "    clean_data = cleaning_results.get(\"cleaned_data\", [])\n",
    "    \n",
    "    # è°ƒç”¨åˆ†æå­å›¾\n",
    "    analysis_subgraph = create_analysis_subgraph()\n",
    "    analysis_result = analysis_subgraph.invoke({\n",
    "        \"clean_data\": clean_data\n",
    "    })\n",
    "    \n",
    "    return {\"analysis_results\": analysis_result}\n",
    "\n",
    "def compile_report(state: AnalysisSystemState) -> AnalysisSystemState:\n",
    "    input_data = state.get(\"input_data\", [])\n",
    "    cleaning_results = state.get(\"cleaning_results\", {})\n",
    "    analysis_results = state.get(\"analysis_results\", {})\n",
    "    \n",
    "    final_report = {\n",
    "        \"summary\": {\n",
    "            \"original_items\": len(input_data),\n",
    "            \"cleaned_items\": len(cleaning_results.get(\"cleaned_data\", [])),\n",
    "            \"data_quality_score\": 85  # æ¨¡æ‹Ÿè¯„åˆ†\n",
    "        },\n",
    "        \"cleaning\": cleaning_results.get(\"cleaning_report\", {}),\n",
    "        \"statistics\": analysis_results.get(\"statistics\", {}),\n",
    "        \"insights\": analysis_results.get(\"insights\", []),\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    print(\"\\nğŸ“„ ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š:\")\n",
    "    print(json.dumps(final_report, ensure_ascii=False, indent=2))\n",
    "    \n",
    "    return {\"final_report\": final_report}\n",
    "\n",
    "# åˆ›å»ºä¸»åˆ†æç³»ç»Ÿ\n",
    "def create_analysis_system():\n",
    "    system_graph = StateGraph(AnalysisSystemState)\n",
    "    \n",
    "    system_graph.add_node(\"load\", load_data)\n",
    "    system_graph.add_node(\"clean\", execute_cleaning)\n",
    "    system_graph.add_node(\"analyze\", execute_analysis)\n",
    "    system_graph.add_node(\"report\", compile_report)\n",
    "    \n",
    "    system_graph.set_entry_point(\"load\")\n",
    "    system_graph.add_edge(\"load\", \"clean\")\n",
    "    system_graph.add_edge(\"clean\", \"analyze\")\n",
    "    system_graph.add_edge(\"analyze\", \"report\")\n",
    "    system_graph.add_edge(\"report\", END)\n",
    "    \n",
    "    return system_graph.compile()\n",
    "\n",
    "# æµ‹è¯•å®Œæ•´ç³»ç»Ÿ\n",
    "print(\"\\n=== å¤šé˜¶æ®µæ•°æ®åˆ†æç³»ç»Ÿæ¼”ç¤º ===\")\n",
    "analysis_system = create_analysis_system()\n",
    "\n",
    "system_result = analysis_system.invoke({})\n",
    "\n",
    "print(\"\\n=== ç³»ç»Ÿæ‰§è¡Œå®Œæˆ ===\")\n",
    "report = system_result.get(\"final_report\", {})\n",
    "print(f\"æ•°æ®è´¨é‡è¯„åˆ†: {report.get('summary', {}).get('data_quality_score', 'N/A')}\")\n",
    "print(f\"å¤„ç†æ•ˆç‡: {report.get('summary', {}).get('cleaned_items', 0)}/{report.get('summary', {}).get('original_items', 0)}\")\n",
    "print(f\"å…³é”®æ´å¯Ÿ: {', '.join(report.get('insights', []))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ç»ƒä¹ é¢˜\n",
    "\n",
    "### ç»ƒä¹ 1ï¼šç”µå•†è®¢å•å¤„ç†ç³»ç»Ÿ\n",
    "è®¾è®¡ä¸€ä¸ªåŒ…å«å¤šä¸ªå­å›¾çš„ç”µå•†è®¢å•å¤„ç†ç³»ç»Ÿï¼š\n",
    "- è®¢å•éªŒè¯å­å›¾\n",
    "- åº“å­˜ç®¡ç†å­å›¾  \n",
    "- æ”¯ä»˜å¤„ç†å­å›¾\n",
    "- ç‰©æµå®‰æ’å­å›¾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç»ƒä¹ 1ï¼šè¯·å®ç°ç”µå•†è®¢å•å¤„ç†ç³»ç»Ÿ\n",
    "# TODO: å®šä¹‰å„ä¸ªå­å›¾çš„çŠ¶æ€å’ŒèŠ‚ç‚¹\n",
    "# TODO: å®ç°å­å›¾ä¹‹é—´çš„çŠ¶æ€ä¼ é€’\n",
    "# TODO: å¤„ç†å¼‚å¸¸æƒ…å†µå’Œå›æ»šé€»è¾‘\n",
    "\n",
    "print(\"è¯·å®ç°ç”µå•†è®¢å•å¤„ç†ç³»ç»Ÿ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ç»ƒä¹ 2ï¼šæ–‡æ¡£å¤„ç†æµæ°´çº¿\n",
    "åˆ›å»ºä¸€ä¸ªæ–‡æ¡£å¤„ç†ç³»ç»Ÿï¼ŒåŒ…å«ï¼š\n",
    "- æ–‡æ¡£è§£æå­å›¾\n",
    "- å†…å®¹æå–å­å›¾\n",
    "- æ ¼å¼è½¬æ¢å­å›¾\n",
    "- è´¨é‡æ£€æŸ¥å­å›¾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç»ƒä¹ 2ï¼šè¯·å®ç°æ–‡æ¡£å¤„ç†æµæ°´çº¿\n",
    "# TODO: è®¾è®¡æ¨¡å—åŒ–çš„æ–‡æ¡£å¤„ç†æµç¨‹\n",
    "# TODO: å®ç°ä¸åŒæ ¼å¼çš„å¤„ç†å­å›¾\n",
    "# TODO: æ·»åŠ é”™è¯¯æ¢å¤å’Œè´¨é‡ä¿è¯æœºåˆ¶\n",
    "\n",
    "print(\"è¯·å®ç°æ–‡æ¡£å¤„ç†æµæ°´çº¿\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ€»ç»“\n",
    "\n",
    "åœ¨æœ¬è¯¾ä¸­ï¼Œæˆ‘ä»¬å­¦ä¹ äº†å­å›¾å’ŒåµŒå¥—å›¾çš„æ ¸å¿ƒæ¦‚å¿µï¼š\n",
    "\n",
    "### å…³é”®è¦ç‚¹ï¼š\n",
    "1. **æ¨¡å—åŒ–è®¾è®¡**ï¼šå°†å¤æ‚ç³»ç»Ÿåˆ†è§£ä¸ºç‹¬ç«‹çš„å­ç³»ç»Ÿ\n",
    "2. **çŠ¶æ€éš”ç¦»**ï¼šæ¯ä¸ªå­å›¾ç»´æŠ¤ç‹¬ç«‹çš„çŠ¶æ€ç©ºé—´\n",
    "3. **å¯é‡ç”¨æ€§**ï¼šå­å›¾å¯ä»¥åœ¨å¤šä¸ªåœ°æ–¹å¤ç”¨\n",
    "4. **å±‚æ¬¡ç»“æ„**ï¼šæ”¯æŒå¤šå±‚åµŒå¥—çš„å¤æ‚æ¶æ„\n",
    "5. **çŠ¶æ€ä¼ é€’**ï¼šåˆç†è®¾è®¡çˆ¶å­å›¾ä¹‹é—´çš„ä¿¡æ¯ä¼ é€’\n",
    "\n",
    "### æœ€ä½³å®è·µï¼š\n",
    "- **æ¸…æ™°è¾¹ç•Œ**ï¼šæ˜ç¡®å®šä¹‰å­å›¾çš„èŒè´£å’Œè¾¹ç•Œ\n",
    "- **çŠ¶æ€è®¾è®¡**ï¼šåˆç†è®¾è®¡çŠ¶æ€ç»“æ„ï¼Œé¿å…è¿‡åº¦è€¦åˆ\n",
    "- **é”™è¯¯å¤„ç†**ï¼šåœ¨å­å›¾çº§åˆ«å®ç°é”™è¯¯å¤„ç†å’Œæ¢å¤\n",
    "- **æ€§èƒ½ä¼˜åŒ–**ï¼šè€ƒè™‘å­å›¾è°ƒç”¨çš„å¼€é”€\n",
    "- **æµ‹è¯•ç­–ç•¥**ï¼šåˆ†åˆ«æµ‹è¯•å­å›¾å’Œæ•´ä½“ç³»ç»Ÿ\n",
    "\n",
    "### åº”ç”¨åœºæ™¯ï¼š\n",
    "- å¤§å‹ä¼ä¸šçº§åº”ç”¨\n",
    "- å¾®æœåŠ¡æ¶æ„\n",
    "- æ•°æ®å¤„ç†æµæ°´çº¿\n",
    "- å·¥ä½œæµå¼•æ“\n",
    "- å¤šé˜¶æ®µä»»åŠ¡ç³»ç»Ÿ\n",
    "\n",
    "## ä¸‹ä¸€è¯¾é¢„å‘Š\n",
    "\n",
    "åœ¨ä¸‹ä¸€è¯¾ã€ŠæŒä¹…åŒ–å’Œæ£€æŸ¥ç‚¹ã€‹ä¸­ï¼Œæˆ‘ä»¬å°†å­¦ä¹ ï¼š\n",
    "- æ£€æŸ¥ç‚¹æœºåˆ¶çš„åŸç†\n",
    "- MemorySaver çš„ä½¿ç”¨\n",
    "- æŒä¹…åŒ–å­˜å‚¨é…ç½®\n",
    "- çŠ¶æ€æ¢å¤å’Œå›æº¯\n",
    "- çº¿ç¨‹ç®¡ç†å’Œé…ç½®\n",
    "- ç”Ÿäº§ç¯å¢ƒçš„å¯é æ€§ä¿è¯"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}